启动一个wsgi-server, Proxy，--(ring)--(http_connect)》account, container, object,
=====================================================
1,
    proxyr/server.py
    168行的eventlet.posthooks()
    是什么用？在此之前在什么时候，在什么地方会设置env[‘eventlet.posthooks']
    这个应该是wsgi-server设置的，如果支持这个参数的话，
    就会在完全发送一个response后执行posthooks,
    如果不支持这个参数的话，就不能记录请求的发送量

    http://eventlet.net/doc/modules/wsgi.html

    Non-Standard Extension to Support Post Hooks¶
    Eventlet’s WSGI server supports a non-standard extension to the WSGI
    specification where env['eventlet.posthooks'] contains an array of post
    hooks that will be called after fully sending a response. Each post hook
    is a tuple of (func, args, kwargs) and the func will be called with the
    WSGI environment dictionary, followed by the args and then the kwargs in
    the post hook.

    For example:

    from eventlet import wsgi
    import eventlet

    def hook(env, arg1, arg2, kwarg3=None, kwarg4=None):
        print 'Hook called: %s %s %s %s %s' % (env, arg1, arg2, kwarg3,
                                               kwarg4)

        def hello_world(env, start_response):
            env['eventlet.posthooks'].append(
                                             (hook, ('arg1',
                                                     'arg2'),
                                              {'kwarg3': 3,
                                              'kwarg4': 4}))
            start_response('200 OK', [('Content-Type',
                                       'text/plain')])
            return ['Hello, World!\r\n']

            wsgi.server(eventlet.listen(('', 8090)),
                        hello_world)

            The above code will print the WSGI environment and
            the other passed function arguments for every
            request processed.

            Post hooks are useful when code needs to be
            executed after a response has been fully sent to
            the client (or when the client disconnects early).
            One example is for more accurate logging of
            bandwidth used, as client disconnects use less
            bandwidth than the actual Content-Length.


------------------------------------------------------------
2,

    在AccountC里面，x-account-meta 在此之前在什么时候设置的？如果没有
    设置的话，执行到这里就会自动退出？

    这个是个傻冒问题，哈哈，在没有的情况下，check_meta返回的是None，Account
    的put是不会退出的。

    ??????????
3，
    account_partition, accounts =
self.app.account_ring.get_nodes(self.account_name)
    到这里开始看ring的计算方法，

4,
    为什么选用2的次方来做韦partition,
    在创建ring时，分两种情况，第一种是存在了objct.build
    ，另一种是不存在objct.build，分别怎么处理?

5,
 RingBuilder(18, 3, 1)

list
+---+									+----------------------------+
|dev1(index=device_id)   +dict-id, zone, ip, port, device, weight, meta-                            |
+---+									+-------------------------+--+
|   |
|dev2(index=device_id)   +dict-id, zone, ip, port, device, weight, meta-                            |
+---+
|   |
+---+
|   |
|   |
    |
































    6, reblance后，怎么找到之前的位置


    7,
    how to understand this ?
    Various hashing algorithms were tried. SHA offers better security, but the
    ring doesn’t need to be cryptographically secure and SHA is slower. Murmur
    was much faster, but MD5 was built-in and hash computation is a small
    percentage of the overall request handling time. In all, once it was
    decided the servers wouldn’t be maintaining the rings themselves anyway
    and only doing hash lookups, MD5 was chosen for its general availability,
    good distribution, and adequate speed.




    8,
    今日写了一个wsgi-server

    9,
    from eventlet import timeout
    可以使用with timeout，

    10,
    看了eventlet，看了用eventlet来实现的一个简单的端口转发，

    11,
    common/db.py --182
    how to understand ''
    conn = sqlite3.connect(tmp_db_file, check_same_thread=False,
                           factory=GreenDBConnection, timeout=0)

    print sqlite3.connect.__doc__
    connect(database[, timeout, isolation_level, detect_types, factory])

    Opens a connection to the SQLite database file *database*. You can use
    ":memory:" to open a database connection to a database that resides in
    RAM instead of on disk.
    从上面这点用法里面可以看到，他没有check_same_thread的参赛，
    timeout=0, 为什么要设置为0?

    factory:
    By default, the sqlite3 module uses its Connection class for the connect
    call. You can, however, subclass the Connection class and make connect()
    use your class instead by providing your class for the factory parameter.

    GreenDBConnection是自定义的一个connection，但不知用意为何？

    12,
    一个account里面多少个container，一个container里面多少个Objgect最优

    13,
    swift能动态调整保存的份数，
    好像不能。

    14，
    在swift/trunk/swift/proxy/server.py
    里面，395行，是不是可能存在访问一个没有定义过的变量cache_key?

    不会，看错了，:),这句之前，先判断了下是否有memcache..

    15,
    memcache缓存了一个数据信息，比如container_info, account_info,



		11,
		删除一个container,后又创建一个同名的container？
    conn.executescript("""
            CREATE TABLE account_stat (
                account TEXT,
                created_at TEXT,
                put_timestamp TEXT DEFAULT '0',
                delete_timestamp TEXT DEFAULT '0',
                container_count INTEGER,
                object_count INTEGER DEFAULT 0,
                bytes_used INTEGER DEFAULT 0,
                hash TEXT default '00000000000000000000000000000000',
                id TEXT,
                status TEXT DEFAULT '',
                status_changed_at TEXT DEFAULT '0',
                metadata TEXT DEFAULT ''
            );

            INSERT INTO account_stat (container_count) VALUES (0);
        """)
        """
        Create container table which is specific to the account DB.

        :param conn: DB connection object
        """
        conn.executescript("""
            CREATE TABLE container (
                ROWID INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE,
                put_timestamp TEXT,
                delete_timestamp TEXT,
                object_count INTEGER,
                bytes_used INTEGER,
                deleted INTEGER DEFAULT 0
            );

    conn.executescript("""
            CREATE TABLE object (
                ROWID INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE,
                created_at TEXT,
                size INTEGER,
                content_type TEXT,
                etag TEXT,
                deleted INTEGER DEFAULT 0
            );
    conn.executescript("""
            CREATE TABLE container_stat (
                account TEXT,
                container TEXT,
                created_at TEXT,
                put_timestamp TEXT DEFAULT '0',
                delete_timestamp TEXT DEFAULT '0',
                object_count INTEGER,
                bytes_used INTEGER,
                reported_put_timestamp TEXT DEFAULT '0',
                reported_delete_timestamp TEXT DEFAULT '0',
								reported_object_count INTEGER DEFAULT 0,
                reported_bytes_used INTEGER DEFAULT 0,
                hash TEXT default '00000000000000000000000000000000',
                id TEXT,
                status TEXT DEFAULT '',
                status_changed_at TEXT DEFAULT '0',
                metadata TEXT DEFAULT ''
            );

            INSERT INTO container_stat (object_count, bytes_used)
                VALUES (0, 0);
        """)

				1，put account,(会initialize，），多次put account呢？检查如果存在，就返回accept,


					delete account, broker.delete_db()把account_state中设置状态为删除，并且设置删除时间，状态改变时间，

				2,  put /account/container

						分三种情况，一种是account已经存在的情况, 一种是account不存在的情况，一种是account被删除后的情况
						a,initialize container,
						b,.(pending_file是在put container时创建的)pending_file是怎么创立的？首先会创建一个container这个数据库，接着去更新account，因为account此时还没有所有相当于是deleted的状态，如果此时x-account-overide-deleted为非yes，会（HTTPNotfound）
							如果x-account-override-deleted为True?此时如果没有account，并没有account.db, 怎么执行put_container操作？会引起服务器的异常，？
						c,
						这个和上面类似，只是此时有数据库，

						上面说的这三种情况不存在，因为在put container情况下，首先会检查是否存在account，不存在就直接退出。

				3,
				PENDING_CAP 设置值的依据?

				正常Put container是，先写入到pending里面，等超过pending_cap后写入文件，
				在get container时，会先把pending里面的数据merge到数据库再查询，返回一个迭代器，

				4，


				5,
				猜想outgong_sync, ingoing_sync和数据库的复制有关

				6,
				这个问题可能记录过，就是sqlite数据库表里面记录多少条数据合适，性能最好？这个和一个account里面几个container，一个ontainer里面多少个object有关。
				http://adrianotto.com/2010/09/openstack-os-is-great-for/
				Don’t store unlimited objects per container
				You can store as many objects in a container as you wish. However, your per-object upload latency will increase considerably one you reach a certain point. I found the optimal number of objects per container to be just under one million. This number will vary depending on your equipment, and how heavy of a workload it’s subjected to.

				7,
				为什么container在删除的时候也是采用的insert 操作呢，可能是因为不管是生成，还是删除container时，最后一步的update account，都是put?

				7,
				account的 meta信息是存放在数据库中，是一条记录，

				8,
				基本看完了account的db操作


				9,
				container的get操作为什么要加上delay_denial呢？

				10,
				account的创建是写透三份，

				11,
				acl控制先保留后看？
				在container的put时，为什么要先clean_acls呢？并检查check_metadata呢？
				暂时猜测，clean_acls是进行验证，如果不符合就报错返回不执行。

				12,
				对proxy_server.py里面的get_update_nodes还有疑问？
				理解了，就是为了确保返回和副本数相同的node，并且尽可能是没有error_limit的节点。
				因为在创建，删除object, container时会涉及到上层目录的修改，所以需要get_update_nodes来获取和副本相同的节点。

				todo 看下object的处理，Proxy和object server的处理，
				之后，开始想下它的replication的处理

				13,
				SegmentedIterable
				x-object-mainifest 是什么用/
				这两个功能是来支持大文件的。

				14,
				在Container的GETorHead里面，为什么要先执行，在执行完成后才进行swift.authorize呢？

				15,
				transfer-encoding的作用是什么？
				3.3.2.2. Chunked Transfer Encoding
				Users can upload data without needing to know in advance the amount of data to be
				uploaded. Users can do this by specifying an HTTP header of Transfer-Encoding:
				chunked and not using a Content-Length header. A good use of this feature would
				be doing a DB dump, piping the output through gzip, then piping the data directly into
				OpenStack Object Storage without having to buffer the data to disk to compute the file
				size. If users attempt to upload more that 5GB with this method, the server will close
				the TCP/IP connection after 5GB and purge the customer data from the system. Users
				must take responsibility for ensuring the data they transfer will be less than 5GB or for
				splitting it into 5GB chunks, each in its own storage object. If you have files that are larger
				than 5GB and still want to use Object Storage, you can segment them prior to upload,
				upload them to the same container, and then use a manifest file to allow downloading of a
				concatenated object containing all the segmented objects, concatenated as a single object.

				16,
				有必要，单独使用eventlet来写一个connect的发送文件客户端和然后用已经写好的wsgi上写一个接受文件的应用，
                                这个已经实现，可以考虑添加一个Proxy?
