应该是有一套部署和管理的脚本，和一个最终用户使用的ui
启动一个wsgi-server, Proxy，--(ring)--(http_connect)》account, container, object,
=====================================================
1,
    proxyr/server.py
    168行的eventlet.posthooks()
    是什么用？在此之前在什么时候，在什么地方会设置env[‘eventlet.posthooks']
    这个应该是wsgi-server设置的，如果支持这个参数的话，
    就会在完全发送一个response后执行posthooks,
    如果不支持这个参数的话，就不能记录请求的发送量

    http://eventlet.net/doc/modules/wsgi.html

    Non-Standard Extension to Support Post Hooks¶
    Eventlet’s WSGI server supports a non-standard extension to the WSGI
    specification where env['eventlet.posthooks'] contains an array of post
    hooks that will be called after fully sending a response. Each post hook
    is a tuple of (func, args, kwargs) and the func will be called with the
    WSGI environment dictionary, followed by the args and then the kwargs in
    the post hook.

    For example:

    from eventlet import wsgi
    import eventlet

    def hook(env, arg1, arg2, kwarg3=None, kwarg4=None):
        print 'Hook called: %s %s %s %s %s' % (env, arg1, arg2, kwarg3,
                                               kwarg4)

        def hello_world(env, start_response):
            env['eventlet.posthooks'].append(
                                             (hook, ('arg1',
                                                     'arg2'),
                                              {'kwarg3': 3,
                                              'kwarg4': 4}))
            start_response('200 OK', [('Content-Type',
                                       'text/plain')])
            return ['Hello, World!\r\n']

            wsgi.server(eventlet.listen(('', 8090)),
                        hello_world)

            The above code will print the WSGI environment and
            the other passed function arguments for every
            request processed.

            Post hooks are useful when code needs to be
            executed after a response has been fully sent to
            the client (or when the client disconnects early).
            One example is for more accurate logging of
            bandwidth used, as client disconnects use less
            bandwidth than the actual Content-Length.


------------------------------------------------------------
2,

    在AccountC里面，x-account-meta 在此之前在什么时候设置的？如果没有
    设置的话，执行到这里就会自动退出？

    这个是个傻冒问题，哈哈，在没有的情况下，check_meta返回的是None，Account
    的put是不会退出的。

    ??????????
3，
    account_partition, accounts =
self.app.account_ring.get_nodes(self.account_name)
    到这里开始看ring的计算方法，

4,
    为什么选用2的次方来做韦partition,
    在创建ring时，分两种情况，第一种是存在了objct.build
    ，另一种是不存在objct.build，分别怎么处理?

5,
 RingBuilder(18, 3, 1)

list
+---+									+----------------------------+
|dev1(index=device_id)   +dict-id, zone, ip, port, device, weight, meta-                            |
+---+									+-------------------------+--+
|   |
|dev2(index=device_id)   +dict-id, zone, ip, port, device, weight, meta-                            |
+---+
|   |
+---+
|   |
|   |
    |
































    6, reblance后，怎么找到之前的位置


    7,
    how to understand this ?
    Various hashing algorithms were tried. SHA offers better security, but the
    ring doesn’t need to be cryptographically secure and SHA is slower. Murmur
    was much faster, but MD5 was built-in and hash computation is a small
    percentage of the overall request handling time. In all, once it was
    decided the servers wouldn’t be maintaining the rings themselves anyway
    and only doing hash lookups, MD5 was chosen for its general availability,
    good distribution, and adequate speed.




    8,
    今日写了一个wsgi-server

    9,
    from eventlet import timeout
    可以使用with timeout，

    10,
    看了eventlet，看了用eventlet来实现的一个简单的端口转发，

    11,
    common/db.py --182
    how to understand ''
    conn = sqlite3.connect(tmp_db_file, check_same_thread=False,
                           factory=GreenDBConnection, timeout=0)

    print sqlite3.connect.__doc__
    connect(database[, timeout, isolation_level, detect_types, factory])

    Opens a connection to the SQLite database file *database*. You can use
    ":memory:" to open a database connection to a database that resides in
    RAM instead of on disk.
    从上面这点用法里面可以看到，他没有check_same_thread的参赛，
    timeout=0, 为什么要设置为0?

    factory:
    By default, the sqlite3 module uses its Connection class for the connect
    call. You can, however, subclass the Connection class and make connect()
    use your class instead by providing your class for the factory parameter.

    GreenDBConnection是自定义的一个connection，但不知用意为何？

    12,
    一个account里面多少个container，一个container里面多少个Objgect最优

    13,
    swift能动态调整保存的份数，
    好像不能。

    14，
    在swift/trunk/swift/proxy/server.py
    里面，395行，是不是可能存在访问一个没有定义过的变量cache_key?

    不会，看错了，:),这句之前，先判断了下是否有memcache..

    15,
    memcache缓存了一个数据信息，比如container_info, account_info,



		11,
		删除一个container,后又创建一个同名的container？
    conn.executescript("""
            CREATE TABLE account_stat (
                account TEXT,
                created_at TEXT,
                put_timestamp TEXT DEFAULT '0',
                delete_timestamp TEXT DEFAULT '0',
                container_count INTEGER,
                object_count INTEGER DEFAULT 0,
                bytes_used INTEGER DEFAULT 0,
                hash TEXT default '00000000000000000000000000000000',
                id TEXT,
                status TEXT DEFAULT '',
                status_changed_at TEXT DEFAULT '0',
                metadata TEXT DEFAULT ''
            );

            INSERT INTO account_stat (container_count) VALUES (0);
        """)
        """
        Create container table which is specific to the account DB.

        :param conn: DB connection object
        """
        conn.executescript("""
            CREATE TABLE container (
                ROWID INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE,
                put_timestamp TEXT,
                delete_timestamp TEXT,
                object_count INTEGER,
                bytes_used INTEGER,
                deleted INTEGER DEFAULT 0
            );

    conn.executescript("""
            CREATE TABLE object (
                ROWID INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE,
                created_at TEXT,
                size INTEGER,
                content_type TEXT,
                etag TEXT,
                deleted INTEGER DEFAULT 0
            );
    conn.executescript("""
            CREATE TABLE container_stat (
                account TEXT,
                container TEXT,
                created_at TEXT,
                put_timestamp TEXT DEFAULT '0',
                delete_timestamp TEXT DEFAULT '0',
                object_count INTEGER,
                bytes_used INTEGER,
                reported_put_timestamp TEXT DEFAULT '0',
                reported_delete_timestamp TEXT DEFAULT '0',
								reported_object_count INTEGER DEFAULT 0,
                reported_bytes_used INTEGER DEFAULT 0,
                hash TEXT default '00000000000000000000000000000000',
                id TEXT,
                status TEXT DEFAULT '',
                status_changed_at TEXT DEFAULT '0',
                metadata TEXT DEFAULT ''
            );

            INSERT INTO container_stat (object_count, bytes_used)
                VALUES (0, 0);
        """)

				1，put account,(会initialize，），多次put account呢？检查如果存在，就返回accept,


					delete account, broker.delete_db()把account_state中设置状态为删除，并且设置删除时间，状态改变时间，

				2,  put /account/container

						分三种情况，一种是account已经存在的情况, 一种是account不存在的情况，一种是account被删除后的情况
						a,initialize container,
						b,.(pending_file是在put container时创建的)pending_file是怎么创立的？首先会创建一个container这个数据库，接着去更新account，因为account此时还没有所有相当于是deleted的状态，如果此时x-account-overide-deleted为非yes，会（HTTPNotfound）
							如果x-account-override-deleted为True?此时如果没有account，并没有account.db, 怎么执行put_container操作？会引起服务器的异常，？
						c,
						这个和上面类似，只是此时有数据库，

						上面说的这三种情况不存在，因为在put container情况下，首先会检查是否存在account，不存在就直接退出。

				3,
				PENDING_CAP 设置值的依据?

				正常Put container是，先写入到pending里面，等超过pending_cap后写入文件，
				在get container时，会先把pending里面的数据merge到数据库再查询，返回一个迭代器，

				4，


				5,
				猜想outgong_sync, ingoing_sync和数据库的复制有关

				6,
				这个问题可能记录过，就是sqlite数据库表里面记录多少条数据合适，性能最好？这个和一个account里面几个container，一个ontainer里面多少个object有关。
				http://adrianotto.com/2010/09/openstack-os-is-great-for/
				Don’t store unlimited objects per container
				You can store as many objects in a container as you wish. However, your per-object upload latency will increase considerably one you reach a certain point. I found the optimal number of objects per container to be just under one million. This number will vary depending on your equipment, and how heavy of a workload it’s subjected to.

				7,
				为什么container在删除的时候也是采用的insert 操作呢，可能是因为不管是生成，还是删除container时，最后一步的update account，都是put?

				7,
				account的 meta信息是存放在数据库中，是一条记录，

				8,
				基本看完了account的db操作


				9,
				container的get操作为什么要加上delay_denial呢？

				10,
				account的创建是写透三份，

				11,
				acl控制先保留后看？
				在container的put时，为什么要先clean_acls呢？并检查check_metadata呢？
				暂时猜测，clean_acls是进行验证，如果不符合就报错返回不执行。

				12,
				对proxy_server.py里面的get_update_nodes还有疑问？
				理解了，就是为了确保返回和副本数相同的node，并且尽可能是没有error_limit的节点。
				因为在创建，删除object, container时会涉及到上层目录的修改，所以需要get_update_nodes来获取和副本相同的节点。

				todo 看下object的处理，Proxy和object server的处理，
				之后，开始想下它的replication的处理

				13,
				SegmentedIterable
				x-object-mainifest 是什么用/
				这两个功能是来支持大文件的。

				14,
				在Container的GETorHead里面，为什么要先执行，在执行完成后才进行swift.authorize呢？

				15,
				transfer-encoding的作用是什么？
				3.3.2.2. Chunked Transfer Encoding
				Users can upload data without needing to know in advance the amount of data to be
				uploaded. Users can do this by specifying an HTTP header of Transfer-Encoding:
				chunked and not using a Content-Length header. A good use of this feature would
				be doing a DB dump, piping the output through gzip, then piping the data directly into
				OpenStack Object Storage without having to buffer the data to disk to compute the file
				size. If users attempt to upload more that 5GB with this method, the server will close
				the TCP/IP connection after 5GB and purge the customer data from the system. Users
				must take responsibility for ensuring the data they transfer will be less than 5GB or for
				splitting it into 5GB chunks, each in its own storage object. If you have files that are larger
				than 5GB and still want to use Object Storage, you can segment them prior to upload,
				upload them to the same container, and then use a manifest file to allow downloading of a
				concatenated object containing all the segmented objects, concatenated as a single object.

				16,
				有必要，单独使用eventlet来写一个connect的发送文件客户端和然后用已经写好的wsgi上写一个接受文件的应用，
                                这个已经实现，可以考虑添加一个Proxy?

																要考虑下，swfit对httplib.Connection 做包装是为什么？但优先看下复制过程。
																                                replication partion/partion_last_3/MD5sum/**
																																replication的基本单元应该是partion/partion_last_3 
																																并且mds计算的只是文件的名字，如果md5不对，就会进行rsync
				17,


				18,
				文件名能否为中文，

				19,
				一个文件对象是怎么保存元数据信息的?.meta, .ts, .data的含义？

				20,
				在recalculate_hashes时会锁住一个文件夹，锁住后是不能往里面写数据了吗？为什么我锁住了，还能用shell去写数据，
			原来在linux 中，锁是建议锁，只有两者都调用flock时才会生效，比如下面的例子
			example$ flock -e lv www_www
			example$ cat /usr/bin/www_www 

			for i in `seq 100`
			do
			echo `date`
			sleep 1
			done

			21,
			查看下account,container的replication, 之后看看auditor和Update ,


			12
			在一个磁盘或一个节点宕机后，进行数据的修复（即）保证数据的份数是三份。
			的时间间隔是？


				17,
				当前的版本中Diskfile每次读的时候没有做mds计算，为什么在1.4的时候加入了每次做mds5计算呢？

				18,
				数据库文件后面添加-journal后标示什么意思？


				19,
				container中object记录, account中container记录，里面的表记录可能会存在不一致，确认下在读一个Objcet时是不是尝试读三次，?
				是的，

				20，
				数据存放了三份，如果在读第一份的时候成功，就返回成功，怎么保证读的第一份是正确的数据，而不是老数据？
				暂时认为是有可能的，有待进一步的考证？

				21，
				数据库的复制，每个数据库都有一个唯一的id，并且有一个表来记录这个表（id）和另一个表进行同步的rowid，在复制时采用两种策略，
				第一种，如果两个数据库不在或差别大约50%就把整个数据库rsync过去，在本地进行合并。另一种就是只传输不同的部分。
				sync_id(point)记录的就是rowid，在每次进行复制后，根据当前的rowid重置sync_id，重置sync_id一步是个原子操作吗？会不会存在实际复制的row_id为100,可是等去重置sync_id时变成了101呢？

				解答:
			  针对第一种情况，在进行复制前，已经获得了point,这样在复制完成后，是直接把这个point插入到outputing-sync表中，
				第二种情况，把整个表复制到远程，先复制成一个临时文件，比如xxxx.db.tmp 然后根据当前xxxx.db.tmp里面当前rowid来重置sync_id, 这个操作完成之后，才把这个xxxx.db.tmp 重命名位xxxx.db

				22,应该理解下swift的日志系统，在此之前先看下account reaper, container 的---->sync和updater, obj的(expirer,  这个是1.4版本里面的，先不考虑),updater  --------->common里面的
				日志系统，要知道一个account的当前状态，这个可以从数据库里面查询获得，或某个account的流量，因为现在知道了是从proxy日志里面读，所以先认为理解了这点，接下来看认证，

				23, 日志系统看完，看下认证，
.account_id
    AUTH_2282f516-559f-4966-b239-b5c88829e927
    AUTH_f6f57a3c-33b5-4e85-95a5-a801e67505c8
    AUTH_fea96a36-c177-4ca4-8c7e-b8c715d9d37b
.token_0
.token_1
.token_2
.token_3
.token_4
.token_5
.token_6
    AUTH_tk9d2941b13d524b268367116ef956dee6
.token_7
.token_8
    AUTH_tk93627c6324c64f78be746f1e6a4e3f98
.token_9
.token_a
.token_b
.token_c
.token_d
.token_e
    AUTH_tk0d37d286af2c43ffad06e99112b3ec4e
.token_f
    AUTH_tk766bbde93771489982d8dc76979d11cf
reseller
    .services
    reseller
test
    .services
    tester
    tester3
test2
    .services
    tester2
		http://swift.openstack.org/1.2/development_saio.html

23, 日志系统看完，看下认证，
日志里面怎么统计出，一个account的使用容量，和某个用户的get 次数，put 次数， 和put流量，get 流量



swauth里面的super_admin_key在创建account和给account设置admin时用，身份验证
acl是怎么实现的呢？
		到此要告一段落，，，


		swift@swift-proxy:/etc/swift$ for j in `seq 1 3`;do for i in `seq 1 3`;do swift-ring-builder object.builder add z${j}-192.168.56.1${j}:6000/sdb${i} 100;done;done
		Device z1-192.168.56.11:6000/sdb1_"" with 100.0 weight got id 0
		Device z1-192.168.56.11:6000/sdb2_"" with 100.0 weight got id 1
		Device z1-192.168.56.11:6000/sdb3_"" with 100.0 weight got id 2
		Device z2-192.168.56.12:6000/sdb1_"" with 100.0 weight got id 3
		Device z2-192.168.56.12:6000/sdb2_"" with 100.0 weight got id 4
		Device z2-192.168.56.12:6000/sdb3_"" with 100.0 weight got id 5
		Device z3-192.168.56.13:6000/sdb1_"" with 100.0 weight got id 6
		Device z3-192.168.56.13:6000/sdb2_"" with 100.0 weight got id 7
		Device z3-192.168.56.13:6000/sdb3_"" with 100.0 weight got id 8
		swift@swift-proxy:/etc/swift$ for j in `seq 1 3`;do for i in `seq 1 3`;do swift-ring-builder container.builder add z${j}-192.168.56.1${j}:6001/sdb${i} 100;done;doneDevice z1-192.168.56.11:6001/sdb1_"" with 100.0 weight got id 0b${i} 100;done;dDevice z1-192.168.56.11:6001/sdb2_"" with 100.0 weight got id 1
		Device z1-192.168.56.11:6001/sdb3_"" with 100.0 weight got id 2
		Device z2-192.168.56.12:6001/sdb1_"" with 100.0 weight got id 3
		Device z2-192.168.56.12:6001/sdb2_"" with 100.0 weight got id 4
		Device z2-192.168.56.12:6001/sdb3_"" with 100.0 weight got id 5
		Device z3-192.168.56.13:6001/sdb1_"" with 100.0 weight got id 6
		Device z3-192.168.56.13:6001/sdb2_"" with 100.0 weight got id 7
		Device z3-192.168.56.13:6001/sdb3_"" with 100.0 weight got id 8
		swift@swift-proxy:/etc/swift$ for j in `seq 1 3`;do for i in `seq 1 3`;do swift-ring-builder account.builder add z${j}-192.168.56.1${j}:6002/sdb${i} 100;done;doneDevice z1-192.168.56.11:6002/sdb1_"" with 100.0 weight got id 0db${i} 100;done;Device z1-192.168.56.11:6002/sdb2_"" with 100.0 weight got id 1
		Device z1-192.168.56.11:6002/sdb3_"" with 100.0 weight got id 2
		Device z2-192.168.56.12:6002/sdb1_"" with 100.0 weight got id 3
		Device z2-192.168.56.12:6002/sdb2_"" with 100.0 weight got id 4
		Device z2-192.168.56.12:6002/sdb3_"" with 100.0 weight got id 5
		Device z3-192.168.56.13:6002/sdb1_"" with 100.0 weight got id 6
		Device z3-192.168.56.13:6002/sdb2_"" with 100.0 weight got id 7
		Device z3-192.168.56.13:6002/sdb3_"" with 100.0 weight got id 8
		swift@swift-proxy:/etc/swift$ 



		root@swift003:~# for i in `seq 1 3`;do dd if=/dev/zero of=/srv/sdb${i} bs=1024 count=0 seek=500000;mkfs.xfs -i size=1024 /srv/sdb${i};mkdir -p /srv/node/sdb${i};done;chown -R swift:swift /srv/node



		swift1.4.4时，官方的多接点部署，需要使用下面的配置文件，关键是在swauth里面添加上default_swift_cluster = local这么行，
		Accept-Ranges: bytes
"""
		root@swift-proxy:/swauth# cat /etc/swift/proxy-server.conf 
		[DEFAULT]
		cert_file = /etc/swift/cert.crt
		key_file = /etc/swift/cert.key
		bind_port = 8080
		workers = 8
		user = swift

		[pipeline:main]
#pipeline = healthcheck cache tempauth proxy-server
		pipeline = healthcheck cache swauth proxy-server

		[app:proxy-server]
		use = egg:swift#proxy
		allow_account_management = true

		[filter:swauth]
		use = egg:swauth#swauth
		set log_name = swauth
		super_admin_key = swauthkey_zz2
		default_swift_cluster = local#https://192.168.56.10:8080/v1



		[filter:tempauth]
		use = egg:swift#tempauth
		user_system_root = testpass .admin https://192.168.56.10:8080/v1/AUTH_system

		[filter:healthcheck]
		use = egg:swift#healthcheck

		[filter:cache]
		use = egg:swift#memcache
		memcache_servers = 192.168.56.10:11211
"""


db_replicator:

run_once -->

roundrobin_datadirs 来收集要执行replicator的db文件，   ---》 _replicate_object 来处理每一个文件。
self._repl_to_node(node, broker, partition, info)
根据node, partion等给节点发送一个replicator的请求，
response = http.replicate('sync', info['max_row'], info['hash'],
                info['id'], info['created_at'], info['put_timestamp'],
                info['delete_timestamp'], info['metadata'])

					服务器端接受这个请求，调用dispatch来执行sync
							return getattr(self, op)(self.broker_class(db_file), args)
					更新数据表的状态，
							if info['put_timestamp'] != put_timestamp or \
                    info['created_at'] != created_at or \
                    info['delete_timestamp'] != delete_timestamp:
							broker.merge_timestamps(
									created_at, put_timestamp, delete_timestamp)
					执行merge_syncs：
							if hash_ == info['hash'] and info['point'] < remote_sync:
									broker.merge_syncs([{'remote_id': id_,
                                 'sync_point': remote_sync}])
							info['point'] = remote_sync
							return Response(simplejson.dumps(info))
							设置remote_id 和sync_point
							然后返回该数据的Info

然后根据response的结果，进行_rsync_db或_usync_db
    if not response:
            return False
        elif response.status == HTTPNotFound.code:  # completely missing, rsync
            self.stats['rsync'] += 1
            return self._rsync_db(broker, node, http, info['id'])
        elif response.status == HTTPInsufficientStorage.code:
            raise DriveNotMounted()
        elif response.status >= 200 and response.status < 300:
            rinfo = simplejson.loads(response.data)
            local_sync = broker.get_sync(rinfo['id'], incoming=False)
            if self._in_sync(rinfo, info, broker, local_sync):
                return True
            # if the difference in rowids between the two differs by
            # more than 50%, rsync then do a remote merge.
            if rinfo['max_row'] / float(info['max_row']) < 0.5:
                self.stats['remote_merge'] += 1
                return self._rsync_db(broker, node, http, info['id'],
                        replicate_method='rsync_then_merge',
                        replicate_timeout=(info['count'] / 2000))
            # else send diffs over to the remote server
            return self._usync_db(max(rinfo['point'], local_sync),
                        broker, http, rinfo['id'], info['id'])

